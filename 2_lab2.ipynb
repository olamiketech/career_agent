{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Second Lab - Week 1, Day 3\n",
    "\n",
    "Today we will work with lots of models! This is a way to get comfortable with APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Important point - please read</h2>\n",
    "            <span style=\"color:#ff7800;\">The way I collaborate with you may be different to other courses you've taken. I prefer not to type code while you watch. Rather, I execute Jupyter Labs, like this, and give you an intuition for what's going on. My suggestion is that you carefully execute this yourself, <b>after</b> watching the lecture. Add print statements to understand what's going on, and then come up with your own variations.<br/><br/>If you have time, I'd love it if you submit a PR for changes in the community_contributions folder - instructions in the resources. Also, if you have a Github account, use this to showcase your variations. Not only is this essential practice, but it demonstrates your skills to others, including perhaps future clients or employers...\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key not set (and this is optional)\n",
      "DeepSeek API Key exists and begins sk-\n",
      "Groq API Key not set (and this is optional)\n"
     ]
    }
   ],
   "source": [
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. Answer only with the question, no explanation.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How would you design an ethical framework for deploying artificial intelligence in a way that balances innovation, societal impact, and individual privacy, and what specific examples would you provide to illustrate the potential trade-offs in real-world applications?\n"
     ]
    }
   ],
   "source": [
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Designing an ethical framework for deploying artificial intelligence (AI) requires careful consideration of various factors, including innovation, societal impact, and individual privacy. The framework should promote responsible AI development and deployment while ensuring fair use and protecting individual rights.\n",
       "\n",
       "### Ethical Framework for AI Deployment\n",
       "\n",
       "1. **Principles of Responsible AI**:\n",
       "   - **Transparency**: AI systems should be explainable and their workings understood by users and stakeholders.\n",
       "   - **Fairness**: AI should be designed to minimize bias and discrimination against individuals or groups, enhancing equity.\n",
       "   - **Accountability**: Organizations deploying AI must be held accountable for the impacts of their systems, with clear lines of responsibility.\n",
       "   - **Privacy Protection**: AI systems should uphold individual privacy rights, adhering to privacy regulations and ethical standards.\n",
       "   - **Societal Well-being**: Consideration of how AI affects society, with a focus on promoting social good and minimizing harm.\n",
       "\n",
       "2. **Stakeholder Involvement**:\n",
       "   - Engage a diverse array of stakeholders, including ethicists, technologists, community representatives, and policymakers, to ensure a broad range of perspectives is considered in AI development.\n",
       "\n",
       "3. **Regulatory and Policy Frameworks**:\n",
       "   - Collaborate with governments and regulatory bodies to establish clear guidelines and standards for the ethical deployment of AI technologies.\n",
       "\n",
       "4. **Impact Assessment**:\n",
       "   - Conduct regular assessments of AI systems regarding their societal impact, privacy implications, and potential biases. These assessments should be comprehensive and periodic, allowing for adjustments over time based on findings.\n",
       "\n",
       "5. **Technology and Algorithms Audits**:\n",
       "   - Implement standard practices for auditing algorithms to ensure that they are functioning as intended, are unbiased, and respect user privacy.\n",
       "\n",
       "6. **User Empowerment**:\n",
       "   - Provide users with control over their data and the ability to opt-out of data collection or algorithmic decision-making when feasible.\n",
       "\n",
       "### Illustrative Examples of Trade-offs\n",
       "\n",
       "1. **Healthcare AI**:\n",
       "   - **Trade-off**: AI can enhance diagnostic accuracy and personalized treatment (innovation) but may require large amounts of sensitive patient data (privacy concerns).\n",
       "   - **Example**: An AI system trained on extensive medical datasets can identify early-stage diseases with high accuracy. However, the use of patient data raises ethical concerns about consent and privacy. Solutions could involve employing anonymization techniques and ensuring the data is used in compliance with health regulations such as HIPAA in the U.S.\n",
       "\n",
       "2. **Facial Recognition Technology**:\n",
       "   - **Trade-off**: Facial recognition can improve security and efficiency (innovation) but poses significant risks to privacy and can lead to surveillance abuses (societal impact).\n",
       "   - **Example**: Cities implementing facial recognition for public safety need to balance the benefits of reducing crime with the potential chilling effects on public behavior and individual privacy. Regulatory frameworks could restrict use cases, such as banning its use in public spaces without due cause.\n",
       "\n",
       "3. **Autonomous Vehicles**:\n",
       "   - **Trade-off**: Self-driving cars could reduce accidents caused by human error (innovation) but raise questions about liability in case of accidents (accountability) and data collection from vehicle users (privacy).\n",
       "   - **Example**: An autonomous vehicle requires data on user habits for optimization, but this raises concerns over how this data is stored, shared, and protected. Frameworks could ensure users are informed about data usage and have control over their information.\n",
       "\n",
       "4. **Recommendation Systems**:\n",
       "   - **Trade-off**: Recommendation algorithms can personalize user experiences and enhance engagement (innovation) but might inadvertently create echo chambers (societal impact) or misuse personal data (privacy).\n",
       "   - **Example**: While recommending movies or products based on user behavior increases sales and user satisfaction, it can lead to reduced exposure to diverse viewpoints. Frameworks can encourage alternative recommendations and emphasize diversity in algorithmic choices.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "An ethical framework for deploying AI must encapsulate a multi-faceted approach, balancing innovation with societal and individual implications. By establishing principles, promoting stakeholder involvement, creating regulatory standards, and illustrating potential trade-offs through real-world examples, organizations can cultivate a responsible AI ecosystem that benefits individuals and society as a whole."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The API we know well\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Ethical Framework for AI Deployment\n",
       "\n",
       "## Core Principles\n",
       "\n",
       "1. **Beneficence**: AI systems should create demonstrable benefits for individuals and society\n",
       "2. **Non-maleficence**: Prevent foreseeable harms and minimize unintended consequences\n",
       "3. **Autonomy**: Respect human agency and decision-making authority\n",
       "4. **Justice**: Ensure equitable distribution of benefits and burdens\n",
       "5. **Transparency**: Make AI systems explainable and their operations understandable\n",
       "6. **Privacy**: Protect personal data and provide meaningful control over its use\n",
       "\n",
       "## Implementation Structure\n",
       "\n",
       "### Pre-Deployment\n",
       "- **Impact assessment**: Evaluate potential social consequences across diverse populations\n",
       "- **Participatory design**: Include stakeholders from affected communities\n",
       "- **Formal review**: Independent ethical evaluation by diverse experts\n",
       "\n",
       "### During Operation\n",
       "- **Continuous monitoring**: Track outcomes and unintended effects\n",
       "- **Feedback mechanisms**: Channels for users to report concerns\n",
       "- **Algorithmic auditing**: Regular evaluation of fairness and bias\n",
       "\n",
       "### Governance\n",
       "- **Clear accountability**: Defined responsibility for decisions and outcomes\n",
       "- **Sunset provisions**: Mandatory reassessment after defined periods\n",
       "- **Tiered oversight**: Proportional scrutiny based on risk level\n",
       "\n",
       "## Real-World Trade-Off Examples\n",
       "\n",
       "### Healthcare Diagnostics\n",
       "**Trade-off**: Accuracy vs. explainability\n",
       "- A deep learning system for cancer detection achieves 97% accuracy but functions as a \"black box\"\n",
       "- More explainable systems might achieve only 94% accuracy\n",
       "- **Ethical approach**: Use the higher-accuracy system with human oversight, while investing in explainable AI research\n",
       "\n",
       "### Criminal Justice\n",
       "**Trade-off**: Efficiency vs. fairness\n",
       "- Predictive policing algorithms may reduce certain crimes but risk amplifying existing biases\n",
       "- **Ethical approach**: Deploy only in limited contexts with continuous monitoring for disparate impacts and regular retraining with bias mitigation techniques\n",
       "\n",
       "### Smart Cities\n",
       "**Trade-off**: Public safety vs. privacy\n",
       "- Facial recognition in public spaces may help locate missing persons but creates mass surveillance\n",
       "- **Ethical approach**: Narrow deployments with sunset provisions, opt-in features where possible, and guaranteed data deletion\n",
       "\n",
       "### Personalized Education\n",
       "**Trade-off**: Customization vs. data protection\n",
       "- AI tutoring systems can adapt to individual learning styles but require extensive behavioral data\n",
       "- **Ethical approach**: Use synthetic or anonymized data where possible, implement strong data minimization, and provide meaningful options for students/parents\n",
       "\n",
       "The most ethical framework recognizes that these trade-offs require ongoing deliberation rather than one-time decisions, with mechanisms that enable society to continually reassess and adjust as technologies and values evolve."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Anthropic has a slightly different API, and Max Tokens is required\n",
    "\n",
    "model_name = \"claude-3-7-sonnet-latest\"\n",
    "\n",
    "claude = Anthropic()\n",
    "response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "answer = response.content[0].text\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Of course. Designing an ethical framework for AI deployment is a critical challenge that requires a multi-layered approach. A robust framework isn't a single rulebook but a dynamic system of principles, processes, and practices.\n",
       "\n",
       "Here is a proposed design for such a framework, followed by concrete examples of the inherent trade-offs.\n",
       "\n",
       "### Designing the Ethical Framework: The \"PIVOT\" Framework\n",
       "\n",
       "This framework is built on five core pillars, supported by two foundational enablers. It's designed to be iterative and adaptive, not a one-time checklist.\n",
       "\n",
       "**Core Pillars:**\n",
       "\n",
       "1.  **Proportionality & Purpose Limitation**\n",
       "    *   **Principle:** The AI's capabilities and data collection must be strictly proportional to a well-defined, legitimate, and beneficial purpose. The system should not be used for purposes beyond those for which it was initially justified.\n",
       "    *   **Implementation:**\n",
       "        *   **Impact Assessments:** Mandate a \"Proportionality Impact Assessment\" before development, evaluating the necessity and scale of the AI against its goal.\n",
       "        *   **Specification of Purpose:** Clearly document the primary purpose in system design and user agreements. Any significant repurposing triggers a new assessment.\n",
       "\n",
       "2.  **Inclusivity & Justice**\n",
       "    *   **Principle:** AI systems must be designed and monitored to avoid unfair bias and should be accessible and beneficial across diverse populations, not just a privileged few.\n",
       "    *   **Implementation:**\n",
       "        *   **Diverse Teams:** Ensure development teams are multidisciplinary and diverse (ethnically, culturally, by gender, and discipline).\n",
       "        *   **Bias Audits:** Conduct regular, independent audits for discriminatory outcomes across different demographic groups.\n",
       "        *   **Redress Mechanisms:** Create clear, human-supported channels for individuals to challenge AI-driven decisions.\n",
       "\n",
       "3.  **Vigilance & Transparency (Explainability)**\n",
       "    *   **Principle:** Organizations must be vigilant in monitoring for unintended consequences. Where possible, AI decisions should be explainable to users and regulators.\n",
       "    *   **Implementation:**\n",
       "        *   **AI \"Nutrition Labels\":** Develop standardized, easy-to-understand labels explaining the AI's function, data use, accuracy, and known limitations.\n",
       "        *   **Human-in-the-Loop:** Maintain meaningful human oversight for high-stakes decisions (e.g., medical diagnosis, parole decisions).\n",
       "        *   **Continuous Monitoring:** Implement real-time monitoring for performance drift and emergent harmful patterns.\n",
       "\n",
       "4.  **Ownership & Privacy by Design**\n",
       "    *   **Principle:** Individual privacy is a right, not an afterthought. Data ownership and usage rights must be clear, and privacy must be embedded into the system's architecture.\n",
       "    *   **Implementation:**\n",
       "        *   **Data Minimization:** Collect only the data absolutely essential for the stated purpose.\n",
       "        *   **Federated Learning & Differential Privacy:** Use techniques that analyze data without centralizing it (federated learning) or that add statistical noise to protect individuals (differential privacy).\n",
       "        *   **Granular User Control:** Provide users with clear options to access, correct, export, and delete their data.\n",
       "\n",
       "5.  **Trust & Accountability**\n",
       "    *   **Principle:** There must be clear, unambiguous accountability for an AI system's outcomes. Someone must be responsible when things go wrong.\n",
       "    *   **Implementation:**\n",
       "        *   **Accountability Lead:** Designate a specific role or team (e.g., a Chief AI Ethics Officer) responsible for the framework's implementation.\n",
       "        *   **Audit Trails:** Maintain secure and detailed logs of the AI's decisions, the data used, and any human interventions.\n",
       "        *   **Liability Frameworks:** Support the development of legal and insurance frameworks that clarify liability for AI-caused harm.\n",
       "\n",
       "**Foundational Enablers:**\n",
       "\n",
       "*   **Cross-Disciplinary Oversight Boards:** Establish internal and external boards including ethicists, lawyers, sociologists, and community representatives to review high-risk projects.\n",
       "*   **Public Education & Dialogue:** Proactively engage with the public to demystify AI, set realistic expectations, and incorporate societal values into the design process.\n",
       "\n",
       "---\n",
       "\n",
       "### Illustrating the Trade-Offs: Real-World Examples\n",
       "\n",
       "The following examples demonstrate how these principles often exist in tension with one another.\n",
       "\n",
       "#### Example 1: AI in Predictive Policing\n",
       "\n",
       "*   **Innovation/Societal Impact vs. Justice/Privacy**\n",
       "    *   **The Promise (Innovation/Impact):** AI can analyze crime data to predict hotspots, potentially allowing police to prevent crimes and allocate resources more efficiently, leading to safer communities.\n",
       "    *   **The Trade-off (Justice/Privacy):**\n",
       "        *   **Bias Amplification:** If historical crime data reflects biased policing practices (e.g., over-policing in minority neighborhoods), the AI will learn and amplify these biases, leading to a destructive feedback loop. This sacrifices **Justice**.\n",
       "        *   **Privacy Erosion:** Widespread surveillance (e.g., facial recognition, social media monitoring) to feed the predictive model severely compromises individual **Privacy** and can create a chilling effect on free assembly.\n",
       "    *   **Balancing Act:** The framework would demand a **Bias Audit** (Pillar 2) and a **Proportionality Assessment** (Pillar 1). It might conclude that the system can only be deployed if its training data is meticulously debiased, its predictions are used only for resource allocation (not individual suspicion), and its use is governed by strong community oversight (**Oversight Board**).\n",
       "\n",
       "#### Example 2: AI-Powered Personalized Healthcare\n",
       "\n",
       "*   **Innovation vs. Privacy & Justice**\n",
       "    *   **The Promise (Innovation/Impact):** AI can analyze a patient's genomic data, medical records, and real-time health metrics from wearables to predict disease risks and create hyper-personalized treatment plans, revolutionizing medicine.\n",
       "    *   **The Trade-off (Privacy/Justice):**\n",
       "        *   **Extreme Data Sensitivity:** This requires collecting the most intimate data possible, creating a massive **Privacy** risk if breached or misused (e.g., by insurers or employers).\n",
       "        *   **Accessibility Gap:** The high cost of such advanced care could make it a luxury for the wealthy, exacerbating health disparities and sacrificing **Inclusivity and Justice**.\n",
       "    *   **Balancing Act:** The framework would enforce **Privacy by Design** (Pillar 4) through techniques like federated learning, where the AI model is trained on your device without your data ever leaving it. It would also push for policy solutions and business models that ensure equitable access, perhaps by being integrated into public health systems.\n",
       "\n",
       "#### Example 3: Generative AI in Creative Industries\n",
       "\n",
       "*   **Innovation vs. Accountability & Societal Impact**\n",
       "    *   **The Promise (Innovation):** Tools like DALL-E or ChatGPT enable incredible new forms of creativity, democratize content creation, and boost productivity.\n",
       "    *   **The Trade-off (Accountability/Societal Impact):**\n",
       "        *   **Job Displacement:** Widespread automation of creative tasks (graphic design, writing, coding) has a significant **Societal Impact**, potentially displacing skilled workers.\n",
       "        *   **Accountability for Misinformation:** It's difficult to assign **Accountability** (Pillar 5) when a generative AI produces plausible but false information (hallucinations) or is used to create deepfakes and misinformation at scale. Who is liable—the developer, the user, or the platform?\n",
       "    *   **Balancing Act:** The framework would require **Transparency** (Pillar 3) about the AI being a tool and not a human. It would push for **Vigilance** in building safeguards against misuse and for developing **Redress Mechanisms** for those harmed by its output. It also forces a societal conversation about reskilling and new economic models.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "An ethical AI framework is not about preventing innovation but about channeling it responsibly. The **PIVOT** framework provides a structured way to identify, debate, and manage the inevitable trade-offs. The goal is not to find a perfect, conflict-free solution, but to ensure that the choices we make in deploying AI are conscious, deliberate, and aligned with our fundamental human values. The most ethical deployments will be those that transparently acknowledge these tensions and actively work to mitigate the negative consequences while maximizing the benefits for all."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "model_name = \"deepseek-chat\"\n",
    "\n",
    "response = deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the next cell, we will use Ollama\n",
    "\n",
    "Ollama runs a local web service that gives an OpenAI compatible endpoint,  \n",
    "and runs models locally using high performance C++ code.\n",
    "\n",
    "If you don't have Ollama, install it here by visiting https://ollama.com then pressing Download and following the instructions.\n",
    "\n",
    "After it's installed, you should be able to visit here: http://localhost:11434 and see the message \"Ollama is running\"\n",
    "\n",
    "You might need to restart Cursor (and maybe reboot). Then open a Terminal (control+\\`) and run `ollama serve`\n",
    "\n",
    "Useful Ollama commands (run these in the terminal, or with an exclamation mark in this notebook):\n",
    "\n",
    "`ollama pull <model_name>` downloads a model locally  \n",
    "`ollama ls` lists all the models you've downloaded  \n",
    "`ollama rm <model_name>` deletes the specified model from your downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Super important - ignore me at your peril!</h2>\n",
    "            <span style=\"color:#ff7800;\">The model called <b>llama3.3</b> is FAR too large for home computers - it's not intended for personal computing and will consume all your resources! Stick with the nicely sized <b>llama3.2</b> or <b>llama3.2:1b</b> and if you want larger, try llama3.1 or smaller variants of Qwen, Gemma, Phi or DeepSeek. See the <A href=\"https://ollama.com/models\">the Ollama models page</a> for a full list of models and sizes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: ollama server not responding - exit status 1\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpt-4o-mini', 'claude-3-7-sonnet-latest', 'deepseek-chat']\n",
      "['Designing an ethical framework for deploying artificial intelligence (AI) requires careful consideration of various factors, including innovation, societal impact, and individual privacy. The framework should promote responsible AI development and deployment while ensuring fair use and protecting individual rights.\\n\\n### Ethical Framework for AI Deployment\\n\\n1. **Principles of Responsible AI**:\\n   - **Transparency**: AI systems should be explainable and their workings understood by users and stakeholders.\\n   - **Fairness**: AI should be designed to minimize bias and discrimination against individuals or groups, enhancing equity.\\n   - **Accountability**: Organizations deploying AI must be held accountable for the impacts of their systems, with clear lines of responsibility.\\n   - **Privacy Protection**: AI systems should uphold individual privacy rights, adhering to privacy regulations and ethical standards.\\n   - **Societal Well-being**: Consideration of how AI affects society, with a focus on promoting social good and minimizing harm.\\n\\n2. **Stakeholder Involvement**:\\n   - Engage a diverse array of stakeholders, including ethicists, technologists, community representatives, and policymakers, to ensure a broad range of perspectives is considered in AI development.\\n\\n3. **Regulatory and Policy Frameworks**:\\n   - Collaborate with governments and regulatory bodies to establish clear guidelines and standards for the ethical deployment of AI technologies.\\n\\n4. **Impact Assessment**:\\n   - Conduct regular assessments of AI systems regarding their societal impact, privacy implications, and potential biases. These assessments should be comprehensive and periodic, allowing for adjustments over time based on findings.\\n\\n5. **Technology and Algorithms Audits**:\\n   - Implement standard practices for auditing algorithms to ensure that they are functioning as intended, are unbiased, and respect user privacy.\\n\\n6. **User Empowerment**:\\n   - Provide users with control over their data and the ability to opt-out of data collection or algorithmic decision-making when feasible.\\n\\n### Illustrative Examples of Trade-offs\\n\\n1. **Healthcare AI**:\\n   - **Trade-off**: AI can enhance diagnostic accuracy and personalized treatment (innovation) but may require large amounts of sensitive patient data (privacy concerns).\\n   - **Example**: An AI system trained on extensive medical datasets can identify early-stage diseases with high accuracy. However, the use of patient data raises ethical concerns about consent and privacy. Solutions could involve employing anonymization techniques and ensuring the data is used in compliance with health regulations such as HIPAA in the U.S.\\n\\n2. **Facial Recognition Technology**:\\n   - **Trade-off**: Facial recognition can improve security and efficiency (innovation) but poses significant risks to privacy and can lead to surveillance abuses (societal impact).\\n   - **Example**: Cities implementing facial recognition for public safety need to balance the benefits of reducing crime with the potential chilling effects on public behavior and individual privacy. Regulatory frameworks could restrict use cases, such as banning its use in public spaces without due cause.\\n\\n3. **Autonomous Vehicles**:\\n   - **Trade-off**: Self-driving cars could reduce accidents caused by human error (innovation) but raise questions about liability in case of accidents (accountability) and data collection from vehicle users (privacy).\\n   - **Example**: An autonomous vehicle requires data on user habits for optimization, but this raises concerns over how this data is stored, shared, and protected. Frameworks could ensure users are informed about data usage and have control over their information.\\n\\n4. **Recommendation Systems**:\\n   - **Trade-off**: Recommendation algorithms can personalize user experiences and enhance engagement (innovation) but might inadvertently create echo chambers (societal impact) or misuse personal data (privacy).\\n   - **Example**: While recommending movies or products based on user behavior increases sales and user satisfaction, it can lead to reduced exposure to diverse viewpoints. Frameworks can encourage alternative recommendations and emphasize diversity in algorithmic choices.\\n\\n### Conclusion\\n\\nAn ethical framework for deploying AI must encapsulate a multi-faceted approach, balancing innovation with societal and individual implications. By establishing principles, promoting stakeholder involvement, creating regulatory standards, and illustrating potential trade-offs through real-world examples, organizations can cultivate a responsible AI ecosystem that benefits individuals and society as a whole.', '# Ethical Framework for AI Deployment\\n\\n## Core Principles\\n\\n1. **Beneficence**: AI systems should create demonstrable benefits for individuals and society\\n2. **Non-maleficence**: Prevent foreseeable harms and minimize unintended consequences\\n3. **Autonomy**: Respect human agency and decision-making authority\\n4. **Justice**: Ensure equitable distribution of benefits and burdens\\n5. **Transparency**: Make AI systems explainable and their operations understandable\\n6. **Privacy**: Protect personal data and provide meaningful control over its use\\n\\n## Implementation Structure\\n\\n### Pre-Deployment\\n- **Impact assessment**: Evaluate potential social consequences across diverse populations\\n- **Participatory design**: Include stakeholders from affected communities\\n- **Formal review**: Independent ethical evaluation by diverse experts\\n\\n### During Operation\\n- **Continuous monitoring**: Track outcomes and unintended effects\\n- **Feedback mechanisms**: Channels for users to report concerns\\n- **Algorithmic auditing**: Regular evaluation of fairness and bias\\n\\n### Governance\\n- **Clear accountability**: Defined responsibility for decisions and outcomes\\n- **Sunset provisions**: Mandatory reassessment after defined periods\\n- **Tiered oversight**: Proportional scrutiny based on risk level\\n\\n## Real-World Trade-Off Examples\\n\\n### Healthcare Diagnostics\\n**Trade-off**: Accuracy vs. explainability\\n- A deep learning system for cancer detection achieves 97% accuracy but functions as a \"black box\"\\n- More explainable systems might achieve only 94% accuracy\\n- **Ethical approach**: Use the higher-accuracy system with human oversight, while investing in explainable AI research\\n\\n### Criminal Justice\\n**Trade-off**: Efficiency vs. fairness\\n- Predictive policing algorithms may reduce certain crimes but risk amplifying existing biases\\n- **Ethical approach**: Deploy only in limited contexts with continuous monitoring for disparate impacts and regular retraining with bias mitigation techniques\\n\\n### Smart Cities\\n**Trade-off**: Public safety vs. privacy\\n- Facial recognition in public spaces may help locate missing persons but creates mass surveillance\\n- **Ethical approach**: Narrow deployments with sunset provisions, opt-in features where possible, and guaranteed data deletion\\n\\n### Personalized Education\\n**Trade-off**: Customization vs. data protection\\n- AI tutoring systems can adapt to individual learning styles but require extensive behavioral data\\n- **Ethical approach**: Use synthetic or anonymized data where possible, implement strong data minimization, and provide meaningful options for students/parents\\n\\nThe most ethical framework recognizes that these trade-offs require ongoing deliberation rather than one-time decisions, with mechanisms that enable society to continually reassess and adjust as technologies and values evolve.', 'Of course. Designing an ethical framework for AI deployment is a critical challenge that requires a multi-layered approach. A robust framework isn\\'t a single rulebook but a dynamic system of principles, processes, and practices.\\n\\nHere is a proposed design for such a framework, followed by concrete examples of the inherent trade-offs.\\n\\n### Designing the Ethical Framework: The \"PIVOT\" Framework\\n\\nThis framework is built on five core pillars, supported by two foundational enablers. It\\'s designed to be iterative and adaptive, not a one-time checklist.\\n\\n**Core Pillars:**\\n\\n1.  **Proportionality & Purpose Limitation**\\n    *   **Principle:** The AI\\'s capabilities and data collection must be strictly proportional to a well-defined, legitimate, and beneficial purpose. The system should not be used for purposes beyond those for which it was initially justified.\\n    *   **Implementation:**\\n        *   **Impact Assessments:** Mandate a \"Proportionality Impact Assessment\" before development, evaluating the necessity and scale of the AI against its goal.\\n        *   **Specification of Purpose:** Clearly document the primary purpose in system design and user agreements. Any significant repurposing triggers a new assessment.\\n\\n2.  **Inclusivity & Justice**\\n    *   **Principle:** AI systems must be designed and monitored to avoid unfair bias and should be accessible and beneficial across diverse populations, not just a privileged few.\\n    *   **Implementation:**\\n        *   **Diverse Teams:** Ensure development teams are multidisciplinary and diverse (ethnically, culturally, by gender, and discipline).\\n        *   **Bias Audits:** Conduct regular, independent audits for discriminatory outcomes across different demographic groups.\\n        *   **Redress Mechanisms:** Create clear, human-supported channels for individuals to challenge AI-driven decisions.\\n\\n3.  **Vigilance & Transparency (Explainability)**\\n    *   **Principle:** Organizations must be vigilant in monitoring for unintended consequences. Where possible, AI decisions should be explainable to users and regulators.\\n    *   **Implementation:**\\n        *   **AI \"Nutrition Labels\":** Develop standardized, easy-to-understand labels explaining the AI\\'s function, data use, accuracy, and known limitations.\\n        *   **Human-in-the-Loop:** Maintain meaningful human oversight for high-stakes decisions (e.g., medical diagnosis, parole decisions).\\n        *   **Continuous Monitoring:** Implement real-time monitoring for performance drift and emergent harmful patterns.\\n\\n4.  **Ownership & Privacy by Design**\\n    *   **Principle:** Individual privacy is a right, not an afterthought. Data ownership and usage rights must be clear, and privacy must be embedded into the system\\'s architecture.\\n    *   **Implementation:**\\n        *   **Data Minimization:** Collect only the data absolutely essential for the stated purpose.\\n        *   **Federated Learning & Differential Privacy:** Use techniques that analyze data without centralizing it (federated learning) or that add statistical noise to protect individuals (differential privacy).\\n        *   **Granular User Control:** Provide users with clear options to access, correct, export, and delete their data.\\n\\n5.  **Trust & Accountability**\\n    *   **Principle:** There must be clear, unambiguous accountability for an AI system\\'s outcomes. Someone must be responsible when things go wrong.\\n    *   **Implementation:**\\n        *   **Accountability Lead:** Designate a specific role or team (e.g., a Chief AI Ethics Officer) responsible for the framework\\'s implementation.\\n        *   **Audit Trails:** Maintain secure and detailed logs of the AI\\'s decisions, the data used, and any human interventions.\\n        *   **Liability Frameworks:** Support the development of legal and insurance frameworks that clarify liability for AI-caused harm.\\n\\n**Foundational Enablers:**\\n\\n*   **Cross-Disciplinary Oversight Boards:** Establish internal and external boards including ethicists, lawyers, sociologists, and community representatives to review high-risk projects.\\n*   **Public Education & Dialogue:** Proactively engage with the public to demystify AI, set realistic expectations, and incorporate societal values into the design process.\\n\\n---\\n\\n### Illustrating the Trade-Offs: Real-World Examples\\n\\nThe following examples demonstrate how these principles often exist in tension with one another.\\n\\n#### Example 1: AI in Predictive Policing\\n\\n*   **Innovation/Societal Impact vs. Justice/Privacy**\\n    *   **The Promise (Innovation/Impact):** AI can analyze crime data to predict hotspots, potentially allowing police to prevent crimes and allocate resources more efficiently, leading to safer communities.\\n    *   **The Trade-off (Justice/Privacy):**\\n        *   **Bias Amplification:** If historical crime data reflects biased policing practices (e.g., over-policing in minority neighborhoods), the AI will learn and amplify these biases, leading to a destructive feedback loop. This sacrifices **Justice**.\\n        *   **Privacy Erosion:** Widespread surveillance (e.g., facial recognition, social media monitoring) to feed the predictive model severely compromises individual **Privacy** and can create a chilling effect on free assembly.\\n    *   **Balancing Act:** The framework would demand a **Bias Audit** (Pillar 2) and a **Proportionality Assessment** (Pillar 1). It might conclude that the system can only be deployed if its training data is meticulously debiased, its predictions are used only for resource allocation (not individual suspicion), and its use is governed by strong community oversight (**Oversight Board**).\\n\\n#### Example 2: AI-Powered Personalized Healthcare\\n\\n*   **Innovation vs. Privacy & Justice**\\n    *   **The Promise (Innovation/Impact):** AI can analyze a patient\\'s genomic data, medical records, and real-time health metrics from wearables to predict disease risks and create hyper-personalized treatment plans, revolutionizing medicine.\\n    *   **The Trade-off (Privacy/Justice):**\\n        *   **Extreme Data Sensitivity:** This requires collecting the most intimate data possible, creating a massive **Privacy** risk if breached or misused (e.g., by insurers or employers).\\n        *   **Accessibility Gap:** The high cost of such advanced care could make it a luxury for the wealthy, exacerbating health disparities and sacrificing **Inclusivity and Justice**.\\n    *   **Balancing Act:** The framework would enforce **Privacy by Design** (Pillar 4) through techniques like federated learning, where the AI model is trained on your device without your data ever leaving it. It would also push for policy solutions and business models that ensure equitable access, perhaps by being integrated into public health systems.\\n\\n#### Example 3: Generative AI in Creative Industries\\n\\n*   **Innovation vs. Accountability & Societal Impact**\\n    *   **The Promise (Innovation):** Tools like DALL-E or ChatGPT enable incredible new forms of creativity, democratize content creation, and boost productivity.\\n    *   **The Trade-off (Accountability/Societal Impact):**\\n        *   **Job Displacement:** Widespread automation of creative tasks (graphic design, writing, coding) has a significant **Societal Impact**, potentially displacing skilled workers.\\n        *   **Accountability for Misinformation:** It\\'s difficult to assign **Accountability** (Pillar 5) when a generative AI produces plausible but false information (hallucinations) or is used to create deepfakes and misinformation at scale. Who is liable—the developer, the user, or the platform?\\n    *   **Balancing Act:** The framework would require **Transparency** (Pillar 3) about the AI being a tool and not a human. It would push for **Vigilance** in building safeguards against misuse and for developing **Redress Mechanisms** for those harmed by its output. It also forces a societal conversation about reskilling and new economic models.\\n\\n### Conclusion\\n\\nAn ethical AI framework is not about preventing innovation but about channeling it responsibly. The **PIVOT** framework provides a structured way to identify, debate, and manage the inevitable trade-offs. The goal is not to find a perfect, conflict-free solution, but to ensure that the choices we make in deploying AI are conscious, deliberate, and aligned with our fundamental human values. The most ethical deployments will be those that transparently acknowledge these tensions and actively work to mitigate the negative consequences while maximizing the benefits for all.']\n"
     ]
    }
   ],
   "source": [
    "# So where are we?\n",
    "\n",
    "print(competitors)\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competitor: gpt-4o-mini\n",
      "\n",
      "Designing an ethical framework for deploying artificial intelligence (AI) requires careful consideration of various factors, including innovation, societal impact, and individual privacy. The framework should promote responsible AI development and deployment while ensuring fair use and protecting individual rights.\n",
      "\n",
      "### Ethical Framework for AI Deployment\n",
      "\n",
      "1. **Principles of Responsible AI**:\n",
      "   - **Transparency**: AI systems should be explainable and their workings understood by users and stakeholders.\n",
      "   - **Fairness**: AI should be designed to minimize bias and discrimination against individuals or groups, enhancing equity.\n",
      "   - **Accountability**: Organizations deploying AI must be held accountable for the impacts of their systems, with clear lines of responsibility.\n",
      "   - **Privacy Protection**: AI systems should uphold individual privacy rights, adhering to privacy regulations and ethical standards.\n",
      "   - **Societal Well-being**: Consideration of how AI affects society, with a focus on promoting social good and minimizing harm.\n",
      "\n",
      "2. **Stakeholder Involvement**:\n",
      "   - Engage a diverse array of stakeholders, including ethicists, technologists, community representatives, and policymakers, to ensure a broad range of perspectives is considered in AI development.\n",
      "\n",
      "3. **Regulatory and Policy Frameworks**:\n",
      "   - Collaborate with governments and regulatory bodies to establish clear guidelines and standards for the ethical deployment of AI technologies.\n",
      "\n",
      "4. **Impact Assessment**:\n",
      "   - Conduct regular assessments of AI systems regarding their societal impact, privacy implications, and potential biases. These assessments should be comprehensive and periodic, allowing for adjustments over time based on findings.\n",
      "\n",
      "5. **Technology and Algorithms Audits**:\n",
      "   - Implement standard practices for auditing algorithms to ensure that they are functioning as intended, are unbiased, and respect user privacy.\n",
      "\n",
      "6. **User Empowerment**:\n",
      "   - Provide users with control over their data and the ability to opt-out of data collection or algorithmic decision-making when feasible.\n",
      "\n",
      "### Illustrative Examples of Trade-offs\n",
      "\n",
      "1. **Healthcare AI**:\n",
      "   - **Trade-off**: AI can enhance diagnostic accuracy and personalized treatment (innovation) but may require large amounts of sensitive patient data (privacy concerns).\n",
      "   - **Example**: An AI system trained on extensive medical datasets can identify early-stage diseases with high accuracy. However, the use of patient data raises ethical concerns about consent and privacy. Solutions could involve employing anonymization techniques and ensuring the data is used in compliance with health regulations such as HIPAA in the U.S.\n",
      "\n",
      "2. **Facial Recognition Technology**:\n",
      "   - **Trade-off**: Facial recognition can improve security and efficiency (innovation) but poses significant risks to privacy and can lead to surveillance abuses (societal impact).\n",
      "   - **Example**: Cities implementing facial recognition for public safety need to balance the benefits of reducing crime with the potential chilling effects on public behavior and individual privacy. Regulatory frameworks could restrict use cases, such as banning its use in public spaces without due cause.\n",
      "\n",
      "3. **Autonomous Vehicles**:\n",
      "   - **Trade-off**: Self-driving cars could reduce accidents caused by human error (innovation) but raise questions about liability in case of accidents (accountability) and data collection from vehicle users (privacy).\n",
      "   - **Example**: An autonomous vehicle requires data on user habits for optimization, but this raises concerns over how this data is stored, shared, and protected. Frameworks could ensure users are informed about data usage and have control over their information.\n",
      "\n",
      "4. **Recommendation Systems**:\n",
      "   - **Trade-off**: Recommendation algorithms can personalize user experiences and enhance engagement (innovation) but might inadvertently create echo chambers (societal impact) or misuse personal data (privacy).\n",
      "   - **Example**: While recommending movies or products based on user behavior increases sales and user satisfaction, it can lead to reduced exposure to diverse viewpoints. Frameworks can encourage alternative recommendations and emphasize diversity in algorithmic choices.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "An ethical framework for deploying AI must encapsulate a multi-faceted approach, balancing innovation with societal and individual implications. By establishing principles, promoting stakeholder involvement, creating regulatory standards, and illustrating potential trade-offs through real-world examples, organizations can cultivate a responsible AI ecosystem that benefits individuals and society as a whole.\n",
      "Competitor: claude-3-7-sonnet-latest\n",
      "\n",
      "# Ethical Framework for AI Deployment\n",
      "\n",
      "## Core Principles\n",
      "\n",
      "1. **Beneficence**: AI systems should create demonstrable benefits for individuals and society\n",
      "2. **Non-maleficence**: Prevent foreseeable harms and minimize unintended consequences\n",
      "3. **Autonomy**: Respect human agency and decision-making authority\n",
      "4. **Justice**: Ensure equitable distribution of benefits and burdens\n",
      "5. **Transparency**: Make AI systems explainable and their operations understandable\n",
      "6. **Privacy**: Protect personal data and provide meaningful control over its use\n",
      "\n",
      "## Implementation Structure\n",
      "\n",
      "### Pre-Deployment\n",
      "- **Impact assessment**: Evaluate potential social consequences across diverse populations\n",
      "- **Participatory design**: Include stakeholders from affected communities\n",
      "- **Formal review**: Independent ethical evaluation by diverse experts\n",
      "\n",
      "### During Operation\n",
      "- **Continuous monitoring**: Track outcomes and unintended effects\n",
      "- **Feedback mechanisms**: Channels for users to report concerns\n",
      "- **Algorithmic auditing**: Regular evaluation of fairness and bias\n",
      "\n",
      "### Governance\n",
      "- **Clear accountability**: Defined responsibility for decisions and outcomes\n",
      "- **Sunset provisions**: Mandatory reassessment after defined periods\n",
      "- **Tiered oversight**: Proportional scrutiny based on risk level\n",
      "\n",
      "## Real-World Trade-Off Examples\n",
      "\n",
      "### Healthcare Diagnostics\n",
      "**Trade-off**: Accuracy vs. explainability\n",
      "- A deep learning system for cancer detection achieves 97% accuracy but functions as a \"black box\"\n",
      "- More explainable systems might achieve only 94% accuracy\n",
      "- **Ethical approach**: Use the higher-accuracy system with human oversight, while investing in explainable AI research\n",
      "\n",
      "### Criminal Justice\n",
      "**Trade-off**: Efficiency vs. fairness\n",
      "- Predictive policing algorithms may reduce certain crimes but risk amplifying existing biases\n",
      "- **Ethical approach**: Deploy only in limited contexts with continuous monitoring for disparate impacts and regular retraining with bias mitigation techniques\n",
      "\n",
      "### Smart Cities\n",
      "**Trade-off**: Public safety vs. privacy\n",
      "- Facial recognition in public spaces may help locate missing persons but creates mass surveillance\n",
      "- **Ethical approach**: Narrow deployments with sunset provisions, opt-in features where possible, and guaranteed data deletion\n",
      "\n",
      "### Personalized Education\n",
      "**Trade-off**: Customization vs. data protection\n",
      "- AI tutoring systems can adapt to individual learning styles but require extensive behavioral data\n",
      "- **Ethical approach**: Use synthetic or anonymized data where possible, implement strong data minimization, and provide meaningful options for students/parents\n",
      "\n",
      "The most ethical framework recognizes that these trade-offs require ongoing deliberation rather than one-time decisions, with mechanisms that enable society to continually reassess and adjust as technologies and values evolve.\n",
      "Competitor: deepseek-chat\n",
      "\n",
      "Of course. Designing an ethical framework for AI deployment is a critical challenge that requires a multi-layered approach. A robust framework isn't a single rulebook but a dynamic system of principles, processes, and practices.\n",
      "\n",
      "Here is a proposed design for such a framework, followed by concrete examples of the inherent trade-offs.\n",
      "\n",
      "### Designing the Ethical Framework: The \"PIVOT\" Framework\n",
      "\n",
      "This framework is built on five core pillars, supported by two foundational enablers. It's designed to be iterative and adaptive, not a one-time checklist.\n",
      "\n",
      "**Core Pillars:**\n",
      "\n",
      "1.  **Proportionality & Purpose Limitation**\n",
      "    *   **Principle:** The AI's capabilities and data collection must be strictly proportional to a well-defined, legitimate, and beneficial purpose. The system should not be used for purposes beyond those for which it was initially justified.\n",
      "    *   **Implementation:**\n",
      "        *   **Impact Assessments:** Mandate a \"Proportionality Impact Assessment\" before development, evaluating the necessity and scale of the AI against its goal.\n",
      "        *   **Specification of Purpose:** Clearly document the primary purpose in system design and user agreements. Any significant repurposing triggers a new assessment.\n",
      "\n",
      "2.  **Inclusivity & Justice**\n",
      "    *   **Principle:** AI systems must be designed and monitored to avoid unfair bias and should be accessible and beneficial across diverse populations, not just a privileged few.\n",
      "    *   **Implementation:**\n",
      "        *   **Diverse Teams:** Ensure development teams are multidisciplinary and diverse (ethnically, culturally, by gender, and discipline).\n",
      "        *   **Bias Audits:** Conduct regular, independent audits for discriminatory outcomes across different demographic groups.\n",
      "        *   **Redress Mechanisms:** Create clear, human-supported channels for individuals to challenge AI-driven decisions.\n",
      "\n",
      "3.  **Vigilance & Transparency (Explainability)**\n",
      "    *   **Principle:** Organizations must be vigilant in monitoring for unintended consequences. Where possible, AI decisions should be explainable to users and regulators.\n",
      "    *   **Implementation:**\n",
      "        *   **AI \"Nutrition Labels\":** Develop standardized, easy-to-understand labels explaining the AI's function, data use, accuracy, and known limitations.\n",
      "        *   **Human-in-the-Loop:** Maintain meaningful human oversight for high-stakes decisions (e.g., medical diagnosis, parole decisions).\n",
      "        *   **Continuous Monitoring:** Implement real-time monitoring for performance drift and emergent harmful patterns.\n",
      "\n",
      "4.  **Ownership & Privacy by Design**\n",
      "    *   **Principle:** Individual privacy is a right, not an afterthought. Data ownership and usage rights must be clear, and privacy must be embedded into the system's architecture.\n",
      "    *   **Implementation:**\n",
      "        *   **Data Minimization:** Collect only the data absolutely essential for the stated purpose.\n",
      "        *   **Federated Learning & Differential Privacy:** Use techniques that analyze data without centralizing it (federated learning) or that add statistical noise to protect individuals (differential privacy).\n",
      "        *   **Granular User Control:** Provide users with clear options to access, correct, export, and delete their data.\n",
      "\n",
      "5.  **Trust & Accountability**\n",
      "    *   **Principle:** There must be clear, unambiguous accountability for an AI system's outcomes. Someone must be responsible when things go wrong.\n",
      "    *   **Implementation:**\n",
      "        *   **Accountability Lead:** Designate a specific role or team (e.g., a Chief AI Ethics Officer) responsible for the framework's implementation.\n",
      "        *   **Audit Trails:** Maintain secure and detailed logs of the AI's decisions, the data used, and any human interventions.\n",
      "        *   **Liability Frameworks:** Support the development of legal and insurance frameworks that clarify liability for AI-caused harm.\n",
      "\n",
      "**Foundational Enablers:**\n",
      "\n",
      "*   **Cross-Disciplinary Oversight Boards:** Establish internal and external boards including ethicists, lawyers, sociologists, and community representatives to review high-risk projects.\n",
      "*   **Public Education & Dialogue:** Proactively engage with the public to demystify AI, set realistic expectations, and incorporate societal values into the design process.\n",
      "\n",
      "---\n",
      "\n",
      "### Illustrating the Trade-Offs: Real-World Examples\n",
      "\n",
      "The following examples demonstrate how these principles often exist in tension with one another.\n",
      "\n",
      "#### Example 1: AI in Predictive Policing\n",
      "\n",
      "*   **Innovation/Societal Impact vs. Justice/Privacy**\n",
      "    *   **The Promise (Innovation/Impact):** AI can analyze crime data to predict hotspots, potentially allowing police to prevent crimes and allocate resources more efficiently, leading to safer communities.\n",
      "    *   **The Trade-off (Justice/Privacy):**\n",
      "        *   **Bias Amplification:** If historical crime data reflects biased policing practices (e.g., over-policing in minority neighborhoods), the AI will learn and amplify these biases, leading to a destructive feedback loop. This sacrifices **Justice**.\n",
      "        *   **Privacy Erosion:** Widespread surveillance (e.g., facial recognition, social media monitoring) to feed the predictive model severely compromises individual **Privacy** and can create a chilling effect on free assembly.\n",
      "    *   **Balancing Act:** The framework would demand a **Bias Audit** (Pillar 2) and a **Proportionality Assessment** (Pillar 1). It might conclude that the system can only be deployed if its training data is meticulously debiased, its predictions are used only for resource allocation (not individual suspicion), and its use is governed by strong community oversight (**Oversight Board**).\n",
      "\n",
      "#### Example 2: AI-Powered Personalized Healthcare\n",
      "\n",
      "*   **Innovation vs. Privacy & Justice**\n",
      "    *   **The Promise (Innovation/Impact):** AI can analyze a patient's genomic data, medical records, and real-time health metrics from wearables to predict disease risks and create hyper-personalized treatment plans, revolutionizing medicine.\n",
      "    *   **The Trade-off (Privacy/Justice):**\n",
      "        *   **Extreme Data Sensitivity:** This requires collecting the most intimate data possible, creating a massive **Privacy** risk if breached or misused (e.g., by insurers or employers).\n",
      "        *   **Accessibility Gap:** The high cost of such advanced care could make it a luxury for the wealthy, exacerbating health disparities and sacrificing **Inclusivity and Justice**.\n",
      "    *   **Balancing Act:** The framework would enforce **Privacy by Design** (Pillar 4) through techniques like federated learning, where the AI model is trained on your device without your data ever leaving it. It would also push for policy solutions and business models that ensure equitable access, perhaps by being integrated into public health systems.\n",
      "\n",
      "#### Example 3: Generative AI in Creative Industries\n",
      "\n",
      "*   **Innovation vs. Accountability & Societal Impact**\n",
      "    *   **The Promise (Innovation):** Tools like DALL-E or ChatGPT enable incredible new forms of creativity, democratize content creation, and boost productivity.\n",
      "    *   **The Trade-off (Accountability/Societal Impact):**\n",
      "        *   **Job Displacement:** Widespread automation of creative tasks (graphic design, writing, coding) has a significant **Societal Impact**, potentially displacing skilled workers.\n",
      "        *   **Accountability for Misinformation:** It's difficult to assign **Accountability** (Pillar 5) when a generative AI produces plausible but false information (hallucinations) or is used to create deepfakes and misinformation at scale. Who is liable—the developer, the user, or the platform?\n",
      "    *   **Balancing Act:** The framework would require **Transparency** (Pillar 3) about the AI being a tool and not a human. It would push for **Vigilance** in building safeguards against misuse and for developing **Redress Mechanisms** for those harmed by its output. It also forces a societal conversation about reskilling and new economic models.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "An ethical AI framework is not about preventing innovation but about channeling it responsibly. The **PIVOT** framework provides a structured way to identify, debate, and manage the inevitable trade-offs. The goal is not to find a perfect, conflict-free solution, but to ensure that the choices we make in deploying AI are conscious, deliberate, and aligned with our fundamental human values. The most ethical deployments will be those that transparently acknowledge these tensions and actively work to mitigate the negative consequences while maximizing the benefits for all.\n"
     ]
    }
   ],
   "source": [
    "# It's nice to know how to use \"zip\"\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"Competitor: {competitor}\\n\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's bring this together - note the use of \"enumerate\"\n",
    "\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Response from competitor 1\n",
      "\n",
      "Designing an ethical framework for deploying artificial intelligence (AI) requires careful consideration of various factors, including innovation, societal impact, and individual privacy. The framework should promote responsible AI development and deployment while ensuring fair use and protecting individual rights.\n",
      "\n",
      "### Ethical Framework for AI Deployment\n",
      "\n",
      "1. **Principles of Responsible AI**:\n",
      "   - **Transparency**: AI systems should be explainable and their workings understood by users and stakeholders.\n",
      "   - **Fairness**: AI should be designed to minimize bias and discrimination against individuals or groups, enhancing equity.\n",
      "   - **Accountability**: Organizations deploying AI must be held accountable for the impacts of their systems, with clear lines of responsibility.\n",
      "   - **Privacy Protection**: AI systems should uphold individual privacy rights, adhering to privacy regulations and ethical standards.\n",
      "   - **Societal Well-being**: Consideration of how AI affects society, with a focus on promoting social good and minimizing harm.\n",
      "\n",
      "2. **Stakeholder Involvement**:\n",
      "   - Engage a diverse array of stakeholders, including ethicists, technologists, community representatives, and policymakers, to ensure a broad range of perspectives is considered in AI development.\n",
      "\n",
      "3. **Regulatory and Policy Frameworks**:\n",
      "   - Collaborate with governments and regulatory bodies to establish clear guidelines and standards for the ethical deployment of AI technologies.\n",
      "\n",
      "4. **Impact Assessment**:\n",
      "   - Conduct regular assessments of AI systems regarding their societal impact, privacy implications, and potential biases. These assessments should be comprehensive and periodic, allowing for adjustments over time based on findings.\n",
      "\n",
      "5. **Technology and Algorithms Audits**:\n",
      "   - Implement standard practices for auditing algorithms to ensure that they are functioning as intended, are unbiased, and respect user privacy.\n",
      "\n",
      "6. **User Empowerment**:\n",
      "   - Provide users with control over their data and the ability to opt-out of data collection or algorithmic decision-making when feasible.\n",
      "\n",
      "### Illustrative Examples of Trade-offs\n",
      "\n",
      "1. **Healthcare AI**:\n",
      "   - **Trade-off**: AI can enhance diagnostic accuracy and personalized treatment (innovation) but may require large amounts of sensitive patient data (privacy concerns).\n",
      "   - **Example**: An AI system trained on extensive medical datasets can identify early-stage diseases with high accuracy. However, the use of patient data raises ethical concerns about consent and privacy. Solutions could involve employing anonymization techniques and ensuring the data is used in compliance with health regulations such as HIPAA in the U.S.\n",
      "\n",
      "2. **Facial Recognition Technology**:\n",
      "   - **Trade-off**: Facial recognition can improve security and efficiency (innovation) but poses significant risks to privacy and can lead to surveillance abuses (societal impact).\n",
      "   - **Example**: Cities implementing facial recognition for public safety need to balance the benefits of reducing crime with the potential chilling effects on public behavior and individual privacy. Regulatory frameworks could restrict use cases, such as banning its use in public spaces without due cause.\n",
      "\n",
      "3. **Autonomous Vehicles**:\n",
      "   - **Trade-off**: Self-driving cars could reduce accidents caused by human error (innovation) but raise questions about liability in case of accidents (accountability) and data collection from vehicle users (privacy).\n",
      "   - **Example**: An autonomous vehicle requires data on user habits for optimization, but this raises concerns over how this data is stored, shared, and protected. Frameworks could ensure users are informed about data usage and have control over their information.\n",
      "\n",
      "4. **Recommendation Systems**:\n",
      "   - **Trade-off**: Recommendation algorithms can personalize user experiences and enhance engagement (innovation) but might inadvertently create echo chambers (societal impact) or misuse personal data (privacy).\n",
      "   - **Example**: While recommending movies or products based on user behavior increases sales and user satisfaction, it can lead to reduced exposure to diverse viewpoints. Frameworks can encourage alternative recommendations and emphasize diversity in algorithmic choices.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "An ethical framework for deploying AI must encapsulate a multi-faceted approach, balancing innovation with societal and individual implications. By establishing principles, promoting stakeholder involvement, creating regulatory standards, and illustrating potential trade-offs through real-world examples, organizations can cultivate a responsible AI ecosystem that benefits individuals and society as a whole.\n",
      "\n",
      "# Response from competitor 2\n",
      "\n",
      "# Ethical Framework for AI Deployment\n",
      "\n",
      "## Core Principles\n",
      "\n",
      "1. **Beneficence**: AI systems should create demonstrable benefits for individuals and society\n",
      "2. **Non-maleficence**: Prevent foreseeable harms and minimize unintended consequences\n",
      "3. **Autonomy**: Respect human agency and decision-making authority\n",
      "4. **Justice**: Ensure equitable distribution of benefits and burdens\n",
      "5. **Transparency**: Make AI systems explainable and their operations understandable\n",
      "6. **Privacy**: Protect personal data and provide meaningful control over its use\n",
      "\n",
      "## Implementation Structure\n",
      "\n",
      "### Pre-Deployment\n",
      "- **Impact assessment**: Evaluate potential social consequences across diverse populations\n",
      "- **Participatory design**: Include stakeholders from affected communities\n",
      "- **Formal review**: Independent ethical evaluation by diverse experts\n",
      "\n",
      "### During Operation\n",
      "- **Continuous monitoring**: Track outcomes and unintended effects\n",
      "- **Feedback mechanisms**: Channels for users to report concerns\n",
      "- **Algorithmic auditing**: Regular evaluation of fairness and bias\n",
      "\n",
      "### Governance\n",
      "- **Clear accountability**: Defined responsibility for decisions and outcomes\n",
      "- **Sunset provisions**: Mandatory reassessment after defined periods\n",
      "- **Tiered oversight**: Proportional scrutiny based on risk level\n",
      "\n",
      "## Real-World Trade-Off Examples\n",
      "\n",
      "### Healthcare Diagnostics\n",
      "**Trade-off**: Accuracy vs. explainability\n",
      "- A deep learning system for cancer detection achieves 97% accuracy but functions as a \"black box\"\n",
      "- More explainable systems might achieve only 94% accuracy\n",
      "- **Ethical approach**: Use the higher-accuracy system with human oversight, while investing in explainable AI research\n",
      "\n",
      "### Criminal Justice\n",
      "**Trade-off**: Efficiency vs. fairness\n",
      "- Predictive policing algorithms may reduce certain crimes but risk amplifying existing biases\n",
      "- **Ethical approach**: Deploy only in limited contexts with continuous monitoring for disparate impacts and regular retraining with bias mitigation techniques\n",
      "\n",
      "### Smart Cities\n",
      "**Trade-off**: Public safety vs. privacy\n",
      "- Facial recognition in public spaces may help locate missing persons but creates mass surveillance\n",
      "- **Ethical approach**: Narrow deployments with sunset provisions, opt-in features where possible, and guaranteed data deletion\n",
      "\n",
      "### Personalized Education\n",
      "**Trade-off**: Customization vs. data protection\n",
      "- AI tutoring systems can adapt to individual learning styles but require extensive behavioral data\n",
      "- **Ethical approach**: Use synthetic or anonymized data where possible, implement strong data minimization, and provide meaningful options for students/parents\n",
      "\n",
      "The most ethical framework recognizes that these trade-offs require ongoing deliberation rather than one-time decisions, with mechanisms that enable society to continually reassess and adjust as technologies and values evolve.\n",
      "\n",
      "# Response from competitor 3\n",
      "\n",
      "Of course. Designing an ethical framework for AI deployment is a critical challenge that requires a multi-layered approach. A robust framework isn't a single rulebook but a dynamic system of principles, processes, and practices.\n",
      "\n",
      "Here is a proposed design for such a framework, followed by concrete examples of the inherent trade-offs.\n",
      "\n",
      "### Designing the Ethical Framework: The \"PIVOT\" Framework\n",
      "\n",
      "This framework is built on five core pillars, supported by two foundational enablers. It's designed to be iterative and adaptive, not a one-time checklist.\n",
      "\n",
      "**Core Pillars:**\n",
      "\n",
      "1.  **Proportionality & Purpose Limitation**\n",
      "    *   **Principle:** The AI's capabilities and data collection must be strictly proportional to a well-defined, legitimate, and beneficial purpose. The system should not be used for purposes beyond those for which it was initially justified.\n",
      "    *   **Implementation:**\n",
      "        *   **Impact Assessments:** Mandate a \"Proportionality Impact Assessment\" before development, evaluating the necessity and scale of the AI against its goal.\n",
      "        *   **Specification of Purpose:** Clearly document the primary purpose in system design and user agreements. Any significant repurposing triggers a new assessment.\n",
      "\n",
      "2.  **Inclusivity & Justice**\n",
      "    *   **Principle:** AI systems must be designed and monitored to avoid unfair bias and should be accessible and beneficial across diverse populations, not just a privileged few.\n",
      "    *   **Implementation:**\n",
      "        *   **Diverse Teams:** Ensure development teams are multidisciplinary and diverse (ethnically, culturally, by gender, and discipline).\n",
      "        *   **Bias Audits:** Conduct regular, independent audits for discriminatory outcomes across different demographic groups.\n",
      "        *   **Redress Mechanisms:** Create clear, human-supported channels for individuals to challenge AI-driven decisions.\n",
      "\n",
      "3.  **Vigilance & Transparency (Explainability)**\n",
      "    *   **Principle:** Organizations must be vigilant in monitoring for unintended consequences. Where possible, AI decisions should be explainable to users and regulators.\n",
      "    *   **Implementation:**\n",
      "        *   **AI \"Nutrition Labels\":** Develop standardized, easy-to-understand labels explaining the AI's function, data use, accuracy, and known limitations.\n",
      "        *   **Human-in-the-Loop:** Maintain meaningful human oversight for high-stakes decisions (e.g., medical diagnosis, parole decisions).\n",
      "        *   **Continuous Monitoring:** Implement real-time monitoring for performance drift and emergent harmful patterns.\n",
      "\n",
      "4.  **Ownership & Privacy by Design**\n",
      "    *   **Principle:** Individual privacy is a right, not an afterthought. Data ownership and usage rights must be clear, and privacy must be embedded into the system's architecture.\n",
      "    *   **Implementation:**\n",
      "        *   **Data Minimization:** Collect only the data absolutely essential for the stated purpose.\n",
      "        *   **Federated Learning & Differential Privacy:** Use techniques that analyze data without centralizing it (federated learning) or that add statistical noise to protect individuals (differential privacy).\n",
      "        *   **Granular User Control:** Provide users with clear options to access, correct, export, and delete their data.\n",
      "\n",
      "5.  **Trust & Accountability**\n",
      "    *   **Principle:** There must be clear, unambiguous accountability for an AI system's outcomes. Someone must be responsible when things go wrong.\n",
      "    *   **Implementation:**\n",
      "        *   **Accountability Lead:** Designate a specific role or team (e.g., a Chief AI Ethics Officer) responsible for the framework's implementation.\n",
      "        *   **Audit Trails:** Maintain secure and detailed logs of the AI's decisions, the data used, and any human interventions.\n",
      "        *   **Liability Frameworks:** Support the development of legal and insurance frameworks that clarify liability for AI-caused harm.\n",
      "\n",
      "**Foundational Enablers:**\n",
      "\n",
      "*   **Cross-Disciplinary Oversight Boards:** Establish internal and external boards including ethicists, lawyers, sociologists, and community representatives to review high-risk projects.\n",
      "*   **Public Education & Dialogue:** Proactively engage with the public to demystify AI, set realistic expectations, and incorporate societal values into the design process.\n",
      "\n",
      "---\n",
      "\n",
      "### Illustrating the Trade-Offs: Real-World Examples\n",
      "\n",
      "The following examples demonstrate how these principles often exist in tension with one another.\n",
      "\n",
      "#### Example 1: AI in Predictive Policing\n",
      "\n",
      "*   **Innovation/Societal Impact vs. Justice/Privacy**\n",
      "    *   **The Promise (Innovation/Impact):** AI can analyze crime data to predict hotspots, potentially allowing police to prevent crimes and allocate resources more efficiently, leading to safer communities.\n",
      "    *   **The Trade-off (Justice/Privacy):**\n",
      "        *   **Bias Amplification:** If historical crime data reflects biased policing practices (e.g., over-policing in minority neighborhoods), the AI will learn and amplify these biases, leading to a destructive feedback loop. This sacrifices **Justice**.\n",
      "        *   **Privacy Erosion:** Widespread surveillance (e.g., facial recognition, social media monitoring) to feed the predictive model severely compromises individual **Privacy** and can create a chilling effect on free assembly.\n",
      "    *   **Balancing Act:** The framework would demand a **Bias Audit** (Pillar 2) and a **Proportionality Assessment** (Pillar 1). It might conclude that the system can only be deployed if its training data is meticulously debiased, its predictions are used only for resource allocation (not individual suspicion), and its use is governed by strong community oversight (**Oversight Board**).\n",
      "\n",
      "#### Example 2: AI-Powered Personalized Healthcare\n",
      "\n",
      "*   **Innovation vs. Privacy & Justice**\n",
      "    *   **The Promise (Innovation/Impact):** AI can analyze a patient's genomic data, medical records, and real-time health metrics from wearables to predict disease risks and create hyper-personalized treatment plans, revolutionizing medicine.\n",
      "    *   **The Trade-off (Privacy/Justice):**\n",
      "        *   **Extreme Data Sensitivity:** This requires collecting the most intimate data possible, creating a massive **Privacy** risk if breached or misused (e.g., by insurers or employers).\n",
      "        *   **Accessibility Gap:** The high cost of such advanced care could make it a luxury for the wealthy, exacerbating health disparities and sacrificing **Inclusivity and Justice**.\n",
      "    *   **Balancing Act:** The framework would enforce **Privacy by Design** (Pillar 4) through techniques like federated learning, where the AI model is trained on your device without your data ever leaving it. It would also push for policy solutions and business models that ensure equitable access, perhaps by being integrated into public health systems.\n",
      "\n",
      "#### Example 3: Generative AI in Creative Industries\n",
      "\n",
      "*   **Innovation vs. Accountability & Societal Impact**\n",
      "    *   **The Promise (Innovation):** Tools like DALL-E or ChatGPT enable incredible new forms of creativity, democratize content creation, and boost productivity.\n",
      "    *   **The Trade-off (Accountability/Societal Impact):**\n",
      "        *   **Job Displacement:** Widespread automation of creative tasks (graphic design, writing, coding) has a significant **Societal Impact**, potentially displacing skilled workers.\n",
      "        *   **Accountability for Misinformation:** It's difficult to assign **Accountability** (Pillar 5) when a generative AI produces plausible but false information (hallucinations) or is used to create deepfakes and misinformation at scale. Who is liable—the developer, the user, or the platform?\n",
      "    *   **Balancing Act:** The framework would require **Transparency** (Pillar 3) about the AI being a tool and not a human. It would push for **Vigilance** in building safeguards against misuse and for developing **Redress Mechanisms** for those harmed by its output. It also forces a societal conversation about reskilling and new economic models.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "An ethical AI framework is not about preventing innovation but about channeling it responsibly. The **PIVOT** framework provides a structured way to identify, debate, and manage the inevitable trade-offs. The goal is not to find a perfect, conflict-free solution, but to ensure that the choices we make in deploying AI are conscious, deliberate, and aligned with our fundamental human values. The most ethical deployments will be those that transparently acknowledge these tensions and actively work to mitigate the negative consequences while maximizing the benefits for all.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are judging a competition between 3 competitors.\n",
      "Each model has been given this question:\n",
      "\n",
      "How would you design an ethical framework for deploying artificial intelligence in a way that balances innovation, societal impact, and individual privacy, and what specific examples would you provide to illustrate the potential trade-offs in real-world applications?\n",
      "\n",
      "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
      "Respond with JSON, and only JSON, with the following format:\n",
      "{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}\n",
      "\n",
      "Here are the responses from each competitor:\n",
      "\n",
      "# Response from competitor 1\n",
      "\n",
      "Designing an ethical framework for deploying artificial intelligence (AI) requires careful consideration of various factors, including innovation, societal impact, and individual privacy. The framework should promote responsible AI development and deployment while ensuring fair use and protecting individual rights.\n",
      "\n",
      "### Ethical Framework for AI Deployment\n",
      "\n",
      "1. **Principles of Responsible AI**:\n",
      "   - **Transparency**: AI systems should be explainable and their workings understood by users and stakeholders.\n",
      "   - **Fairness**: AI should be designed to minimize bias and discrimination against individuals or groups, enhancing equity.\n",
      "   - **Accountability**: Organizations deploying AI must be held accountable for the impacts of their systems, with clear lines of responsibility.\n",
      "   - **Privacy Protection**: AI systems should uphold individual privacy rights, adhering to privacy regulations and ethical standards.\n",
      "   - **Societal Well-being**: Consideration of how AI affects society, with a focus on promoting social good and minimizing harm.\n",
      "\n",
      "2. **Stakeholder Involvement**:\n",
      "   - Engage a diverse array of stakeholders, including ethicists, technologists, community representatives, and policymakers, to ensure a broad range of perspectives is considered in AI development.\n",
      "\n",
      "3. **Regulatory and Policy Frameworks**:\n",
      "   - Collaborate with governments and regulatory bodies to establish clear guidelines and standards for the ethical deployment of AI technologies.\n",
      "\n",
      "4. **Impact Assessment**:\n",
      "   - Conduct regular assessments of AI systems regarding their societal impact, privacy implications, and potential biases. These assessments should be comprehensive and periodic, allowing for adjustments over time based on findings.\n",
      "\n",
      "5. **Technology and Algorithms Audits**:\n",
      "   - Implement standard practices for auditing algorithms to ensure that they are functioning as intended, are unbiased, and respect user privacy.\n",
      "\n",
      "6. **User Empowerment**:\n",
      "   - Provide users with control over their data and the ability to opt-out of data collection or algorithmic decision-making when feasible.\n",
      "\n",
      "### Illustrative Examples of Trade-offs\n",
      "\n",
      "1. **Healthcare AI**:\n",
      "   - **Trade-off**: AI can enhance diagnostic accuracy and personalized treatment (innovation) but may require large amounts of sensitive patient data (privacy concerns).\n",
      "   - **Example**: An AI system trained on extensive medical datasets can identify early-stage diseases with high accuracy. However, the use of patient data raises ethical concerns about consent and privacy. Solutions could involve employing anonymization techniques and ensuring the data is used in compliance with health regulations such as HIPAA in the U.S.\n",
      "\n",
      "2. **Facial Recognition Technology**:\n",
      "   - **Trade-off**: Facial recognition can improve security and efficiency (innovation) but poses significant risks to privacy and can lead to surveillance abuses (societal impact).\n",
      "   - **Example**: Cities implementing facial recognition for public safety need to balance the benefits of reducing crime with the potential chilling effects on public behavior and individual privacy. Regulatory frameworks could restrict use cases, such as banning its use in public spaces without due cause.\n",
      "\n",
      "3. **Autonomous Vehicles**:\n",
      "   - **Trade-off**: Self-driving cars could reduce accidents caused by human error (innovation) but raise questions about liability in case of accidents (accountability) and data collection from vehicle users (privacy).\n",
      "   - **Example**: An autonomous vehicle requires data on user habits for optimization, but this raises concerns over how this data is stored, shared, and protected. Frameworks could ensure users are informed about data usage and have control over their information.\n",
      "\n",
      "4. **Recommendation Systems**:\n",
      "   - **Trade-off**: Recommendation algorithms can personalize user experiences and enhance engagement (innovation) but might inadvertently create echo chambers (societal impact) or misuse personal data (privacy).\n",
      "   - **Example**: While recommending movies or products based on user behavior increases sales and user satisfaction, it can lead to reduced exposure to diverse viewpoints. Frameworks can encourage alternative recommendations and emphasize diversity in algorithmic choices.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "An ethical framework for deploying AI must encapsulate a multi-faceted approach, balancing innovation with societal and individual implications. By establishing principles, promoting stakeholder involvement, creating regulatory standards, and illustrating potential trade-offs through real-world examples, organizations can cultivate a responsible AI ecosystem that benefits individuals and society as a whole.\n",
      "\n",
      "# Response from competitor 2\n",
      "\n",
      "# Ethical Framework for AI Deployment\n",
      "\n",
      "## Core Principles\n",
      "\n",
      "1. **Beneficence**: AI systems should create demonstrable benefits for individuals and society\n",
      "2. **Non-maleficence**: Prevent foreseeable harms and minimize unintended consequences\n",
      "3. **Autonomy**: Respect human agency and decision-making authority\n",
      "4. **Justice**: Ensure equitable distribution of benefits and burdens\n",
      "5. **Transparency**: Make AI systems explainable and their operations understandable\n",
      "6. **Privacy**: Protect personal data and provide meaningful control over its use\n",
      "\n",
      "## Implementation Structure\n",
      "\n",
      "### Pre-Deployment\n",
      "- **Impact assessment**: Evaluate potential social consequences across diverse populations\n",
      "- **Participatory design**: Include stakeholders from affected communities\n",
      "- **Formal review**: Independent ethical evaluation by diverse experts\n",
      "\n",
      "### During Operation\n",
      "- **Continuous monitoring**: Track outcomes and unintended effects\n",
      "- **Feedback mechanisms**: Channels for users to report concerns\n",
      "- **Algorithmic auditing**: Regular evaluation of fairness and bias\n",
      "\n",
      "### Governance\n",
      "- **Clear accountability**: Defined responsibility for decisions and outcomes\n",
      "- **Sunset provisions**: Mandatory reassessment after defined periods\n",
      "- **Tiered oversight**: Proportional scrutiny based on risk level\n",
      "\n",
      "## Real-World Trade-Off Examples\n",
      "\n",
      "### Healthcare Diagnostics\n",
      "**Trade-off**: Accuracy vs. explainability\n",
      "- A deep learning system for cancer detection achieves 97% accuracy but functions as a \"black box\"\n",
      "- More explainable systems might achieve only 94% accuracy\n",
      "- **Ethical approach**: Use the higher-accuracy system with human oversight, while investing in explainable AI research\n",
      "\n",
      "### Criminal Justice\n",
      "**Trade-off**: Efficiency vs. fairness\n",
      "- Predictive policing algorithms may reduce certain crimes but risk amplifying existing biases\n",
      "- **Ethical approach**: Deploy only in limited contexts with continuous monitoring for disparate impacts and regular retraining with bias mitigation techniques\n",
      "\n",
      "### Smart Cities\n",
      "**Trade-off**: Public safety vs. privacy\n",
      "- Facial recognition in public spaces may help locate missing persons but creates mass surveillance\n",
      "- **Ethical approach**: Narrow deployments with sunset provisions, opt-in features where possible, and guaranteed data deletion\n",
      "\n",
      "### Personalized Education\n",
      "**Trade-off**: Customization vs. data protection\n",
      "- AI tutoring systems can adapt to individual learning styles but require extensive behavioral data\n",
      "- **Ethical approach**: Use synthetic or anonymized data where possible, implement strong data minimization, and provide meaningful options for students/parents\n",
      "\n",
      "The most ethical framework recognizes that these trade-offs require ongoing deliberation rather than one-time decisions, with mechanisms that enable society to continually reassess and adjust as technologies and values evolve.\n",
      "\n",
      "# Response from competitor 3\n",
      "\n",
      "Of course. Designing an ethical framework for AI deployment is a critical challenge that requires a multi-layered approach. A robust framework isn't a single rulebook but a dynamic system of principles, processes, and practices.\n",
      "\n",
      "Here is a proposed design for such a framework, followed by concrete examples of the inherent trade-offs.\n",
      "\n",
      "### Designing the Ethical Framework: The \"PIVOT\" Framework\n",
      "\n",
      "This framework is built on five core pillars, supported by two foundational enablers. It's designed to be iterative and adaptive, not a one-time checklist.\n",
      "\n",
      "**Core Pillars:**\n",
      "\n",
      "1.  **Proportionality & Purpose Limitation**\n",
      "    *   **Principle:** The AI's capabilities and data collection must be strictly proportional to a well-defined, legitimate, and beneficial purpose. The system should not be used for purposes beyond those for which it was initially justified.\n",
      "    *   **Implementation:**\n",
      "        *   **Impact Assessments:** Mandate a \"Proportionality Impact Assessment\" before development, evaluating the necessity and scale of the AI against its goal.\n",
      "        *   **Specification of Purpose:** Clearly document the primary purpose in system design and user agreements. Any significant repurposing triggers a new assessment.\n",
      "\n",
      "2.  **Inclusivity & Justice**\n",
      "    *   **Principle:** AI systems must be designed and monitored to avoid unfair bias and should be accessible and beneficial across diverse populations, not just a privileged few.\n",
      "    *   **Implementation:**\n",
      "        *   **Diverse Teams:** Ensure development teams are multidisciplinary and diverse (ethnically, culturally, by gender, and discipline).\n",
      "        *   **Bias Audits:** Conduct regular, independent audits for discriminatory outcomes across different demographic groups.\n",
      "        *   **Redress Mechanisms:** Create clear, human-supported channels for individuals to challenge AI-driven decisions.\n",
      "\n",
      "3.  **Vigilance & Transparency (Explainability)**\n",
      "    *   **Principle:** Organizations must be vigilant in monitoring for unintended consequences. Where possible, AI decisions should be explainable to users and regulators.\n",
      "    *   **Implementation:**\n",
      "        *   **AI \"Nutrition Labels\":** Develop standardized, easy-to-understand labels explaining the AI's function, data use, accuracy, and known limitations.\n",
      "        *   **Human-in-the-Loop:** Maintain meaningful human oversight for high-stakes decisions (e.g., medical diagnosis, parole decisions).\n",
      "        *   **Continuous Monitoring:** Implement real-time monitoring for performance drift and emergent harmful patterns.\n",
      "\n",
      "4.  **Ownership & Privacy by Design**\n",
      "    *   **Principle:** Individual privacy is a right, not an afterthought. Data ownership and usage rights must be clear, and privacy must be embedded into the system's architecture.\n",
      "    *   **Implementation:**\n",
      "        *   **Data Minimization:** Collect only the data absolutely essential for the stated purpose.\n",
      "        *   **Federated Learning & Differential Privacy:** Use techniques that analyze data without centralizing it (federated learning) or that add statistical noise to protect individuals (differential privacy).\n",
      "        *   **Granular User Control:** Provide users with clear options to access, correct, export, and delete their data.\n",
      "\n",
      "5.  **Trust & Accountability**\n",
      "    *   **Principle:** There must be clear, unambiguous accountability for an AI system's outcomes. Someone must be responsible when things go wrong.\n",
      "    *   **Implementation:**\n",
      "        *   **Accountability Lead:** Designate a specific role or team (e.g., a Chief AI Ethics Officer) responsible for the framework's implementation.\n",
      "        *   **Audit Trails:** Maintain secure and detailed logs of the AI's decisions, the data used, and any human interventions.\n",
      "        *   **Liability Frameworks:** Support the development of legal and insurance frameworks that clarify liability for AI-caused harm.\n",
      "\n",
      "**Foundational Enablers:**\n",
      "\n",
      "*   **Cross-Disciplinary Oversight Boards:** Establish internal and external boards including ethicists, lawyers, sociologists, and community representatives to review high-risk projects.\n",
      "*   **Public Education & Dialogue:** Proactively engage with the public to demystify AI, set realistic expectations, and incorporate societal values into the design process.\n",
      "\n",
      "---\n",
      "\n",
      "### Illustrating the Trade-Offs: Real-World Examples\n",
      "\n",
      "The following examples demonstrate how these principles often exist in tension with one another.\n",
      "\n",
      "#### Example 1: AI in Predictive Policing\n",
      "\n",
      "*   **Innovation/Societal Impact vs. Justice/Privacy**\n",
      "    *   **The Promise (Innovation/Impact):** AI can analyze crime data to predict hotspots, potentially allowing police to prevent crimes and allocate resources more efficiently, leading to safer communities.\n",
      "    *   **The Trade-off (Justice/Privacy):**\n",
      "        *   **Bias Amplification:** If historical crime data reflects biased policing practices (e.g., over-policing in minority neighborhoods), the AI will learn and amplify these biases, leading to a destructive feedback loop. This sacrifices **Justice**.\n",
      "        *   **Privacy Erosion:** Widespread surveillance (e.g., facial recognition, social media monitoring) to feed the predictive model severely compromises individual **Privacy** and can create a chilling effect on free assembly.\n",
      "    *   **Balancing Act:** The framework would demand a **Bias Audit** (Pillar 2) and a **Proportionality Assessment** (Pillar 1). It might conclude that the system can only be deployed if its training data is meticulously debiased, its predictions are used only for resource allocation (not individual suspicion), and its use is governed by strong community oversight (**Oversight Board**).\n",
      "\n",
      "#### Example 2: AI-Powered Personalized Healthcare\n",
      "\n",
      "*   **Innovation vs. Privacy & Justice**\n",
      "    *   **The Promise (Innovation/Impact):** AI can analyze a patient's genomic data, medical records, and real-time health metrics from wearables to predict disease risks and create hyper-personalized treatment plans, revolutionizing medicine.\n",
      "    *   **The Trade-off (Privacy/Justice):**\n",
      "        *   **Extreme Data Sensitivity:** This requires collecting the most intimate data possible, creating a massive **Privacy** risk if breached or misused (e.g., by insurers or employers).\n",
      "        *   **Accessibility Gap:** The high cost of such advanced care could make it a luxury for the wealthy, exacerbating health disparities and sacrificing **Inclusivity and Justice**.\n",
      "    *   **Balancing Act:** The framework would enforce **Privacy by Design** (Pillar 4) through techniques like federated learning, where the AI model is trained on your device without your data ever leaving it. It would also push for policy solutions and business models that ensure equitable access, perhaps by being integrated into public health systems.\n",
      "\n",
      "#### Example 3: Generative AI in Creative Industries\n",
      "\n",
      "*   **Innovation vs. Accountability & Societal Impact**\n",
      "    *   **The Promise (Innovation):** Tools like DALL-E or ChatGPT enable incredible new forms of creativity, democratize content creation, and boost productivity.\n",
      "    *   **The Trade-off (Accountability/Societal Impact):**\n",
      "        *   **Job Displacement:** Widespread automation of creative tasks (graphic design, writing, coding) has a significant **Societal Impact**, potentially displacing skilled workers.\n",
      "        *   **Accountability for Misinformation:** It's difficult to assign **Accountability** (Pillar 5) when a generative AI produces plausible but false information (hallucinations) or is used to create deepfakes and misinformation at scale. Who is liable—the developer, the user, or the platform?\n",
      "    *   **Balancing Act:** The framework would require **Transparency** (Pillar 3) about the AI being a tool and not a human. It would push for **Vigilance** in building safeguards against misuse and for developing **Redress Mechanisms** for those harmed by its output. It also forces a societal conversation about reskilling and new economic models.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "An ethical AI framework is not about preventing innovation but about channeling it responsibly. The **PIVOT** framework provides a structured way to identify, debate, and manage the inevitable trade-offs. The goal is not to find a perfect, conflict-free solution, but to ensure that the choices we make in deploying AI are conscious, deliberate, and aligned with our fundamental human values. The most ethical deployments will be those that transparently acknowledge these tensions and actively work to mitigate the negative consequences while maximizing the benefits for all.\n",
      "\n",
      "\n",
      "\n",
      "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\n"
     ]
    }
   ],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"results\": [\"3\", \"2\", \"1\"]}\n"
     ]
    }
   ],
   "source": [
    "# Judgement time!\n",
    "\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: deepseek-chat\n",
      "Rank 2: claude-3-7-sonnet-latest\n",
      "Rank 3: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# OK let's turn this into results!\n",
    "\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
    "            <span style=\"color:#ff7800;\">Which pattern(s) did this use? Try updating this to add another Agentic design pattern.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Commercial implications</h2>\n",
    "            <span style=\"color:#00bfff;\">These kinds of patterns - to send a task to multiple models, and evaluate results,\n",
    "            are common where you need to improve the quality of your LLM response. This approach can be universally applied\n",
    "            to business projects where accuracy is critical.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
