{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Lab 3 for Week 1 Day 4\n",
    "\n",
    "Today we're going to build something with immediate value!\n",
    "\n",
    "In the folder `me` I've put a single file `linkedin.pdf` - it's a PDF download of my LinkedIn profile.\n",
    "\n",
    "Please replace it with yours!\n",
    "\n",
    "I've also made a file called `summary.txt`\n",
    "\n",
    "We're not going to use Tools just yet - we're going to add the tool tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Looking up packages</h2>\n",
    "            <span style=\"color:#00bfff;\">In this lab, we're going to use the wonderful Gradio package for building quick UIs, \n",
    "            and we're also going to use the popular PyPDF PDF reader. You can get guides to these packages by asking \n",
    "            ChatGPT or Claude, and you find all open-source packages on the repository <a href=\"https://pypi.org\">https://pypi.org</a>.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"me/linkedin.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "  \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "PROFESSIONAL SUMMARY  \n",
      "Applied AI Engineer with a strong foundation in building intelligent systems using machine learning, LLMs, and backend \n",
      "engineering. Skilled in deploying AI solutions for structured data extraction, image recognition, and document parsing. \n",
      "Proven ability to integrate NLP, cloud computing, and prompt engineering into real-world AI products. Currently leading \n",
      "development on LeafDoctor.org an AI-powered plant health platform using GPT-based assistants and CNN-based visual \n",
      "diagnostics. Passionate about automating complex workflows and transforming outdated manual processes through \n",
      "scalable AI systems. \n",
      " \n",
      "CORE COMPETENCIES \n",
      " Applied AI | LLMs & Prompt Engineering | Backend AI Systems | Deep Learning | NLP | LangChain | Document \n",
      "Parsing \n",
      " Machine Learning | Software Development | MLOps | Reinforcement Learning | Kafka | Cloud Deployment \n",
      "(AWS, GCP, Azure) \n",
      " Python, SQL, R, Java | PyTorch, TensorFlow, Git | REST APIs | Image Classification | CI/CD Integration \n",
      " \n",
      "PROFESSIONAL EXPERIENCE \n",
      "AI Engineer (Freelance) \n",
      "Olamike Solutions | Remote | Nov 2024 – Present \n",
      " Lead Applied AI Engineer for LeafDoctor.org, an AI-powered system for identifying plant diseases via image \n",
      "uploads. \n",
      " Engineered real-time classification pipelines with CNNs for disease detection (92% accuracy). \n",
      " Designed and deployed GPT-4-based voice and chat assistants using LangChain for treatment recommendations \n",
      "and care tips. \n",
      " Built and maintained backend APIs and cloud infrastructure on GCP for image processing, real-time inference, and \n",
      "user interaction. \n",
      " Integrated document parsing and structured data extraction workflows for plant health records and expert \n",
      "guidance. \n",
      "Machine Learning Engineer \n",
      "LeafDoctor (Part-Time) | Jan 2024 – Present \n",
      " Constructed LLM pipelines for plant diagnostic automation using prompt tuning and zero-shot models. \n",
      " Created multi-stage image-to-text-to-decision flow using LangChain and OpenAI APIs. \n",
      " Collaborated with software engineers and domain experts to optimize annotation pipelines and improve model \n",
      "response relevance. \n",
      "Data Scientist \n",
      "DataKirk | Edinburgh, Scotland (Hybrid) | May 2024 – Nov 2024 \n",
      " Developed ML models to predict customer behaviour and optimize workflows across enterprise clients. \n",
      "Michael Salami \n",
      "Stirling, United Kingdom | 07823583837 | \n",
      "olamike077@gmail.com LinkedIn Profile | \n",
      "Portfolio     \n",
      "   \n",
      " \n",
      " Automated extraction and processing of technical documents using NLP, reducing analyst workload by 35%. \n",
      " Applied LLM-based agents to summarize engineering specs and structured data from raw PDF sources. \n",
      "STEM Ambassador \n",
      "CodeBase Stirling | Stirling, Scotland | Feb 2023 – Jan 2024 \n",
      " Delivered workshops and mentorship on LLMs, AI ethics, and hands-on applications in generative AI. \n",
      "Software Engineer \n",
      "Shokenny Solutions Ltd | Akure, Nigeria | Aug 2020 – Dec 2023 \n",
      " Developed full-stack applications using Flutter, Python, and Java, increasing delivery throughput by 98%. \n",
      " Built backend systems supporting customer data extraction, transformation, and analytics at scale. \n",
      " Translated client business needs into deployable technical architectures across 10+ enterprise projects. \n",
      " \n",
      "SELECTED PROJECTS  \n",
      "ElectroGadget AI Chatbot \n",
      " Built LLM chatbot using GPT-4, LangChain, and pdfplumber to resolve customer queries in retail domain.  \n",
      " Automated document reading and Q&A over manuals and specs; increased support response accuracy by 25%.  \n",
      " \n",
      "Store Performance Prediction \n",
      " Trained and deployed ensemble ML models to forecast retail KPIs with 88.7% accuracy.  \n",
      " \n",
      "Environmental Data Modeling \n",
      " Used PyTorch and remote sensing data to optimize environmental predictions for sustainability applications. \n",
      " \n",
      "Sports Analytics Platform \n",
      " Built linear regression models to predict athlete metrics; integrated hypothesis testing and feature selection. \n",
      " \n",
      "CERTIFICATIONS \n",
      "A.I. & Machine Learning Data Science Bootcamp:    Zero to Mastery Academy                     2024 \n",
      "R Programming A-Z: R for Data Science with Real Exercises:   Udemy                                     2024 \n",
      "LangChain for LLM Application Development:   DeepLearning.AI              2024 \n",
      "Introduction to Cybersecurity:     Cisco               2020 \n",
      "Awards \n",
      " \n",
      "Future Tech Star:       CODEBASE     2024 \n",
      " Recognized for innovative leadership in AI product development and mentoring. \n",
      " \n",
      "ADDITIONAL INFO \n",
      "Remote, Hybrid or Onsite Work Ready | Willing to Travel | Availability: Immediate | Visa Sponsorship Ready \n",
      "Skilled with Git, CI/CD, LangChain, Kafka, REST APIs, and cloud-scale deployments \n",
      " \n",
      "References \n",
      "Available upon request. \n"
     ]
    }
   ],
   "source": [
    "print(linkedin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Michael Salami\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are acting as Michael Salami. You are answering questions on Michael Salami's website, particularly questions related to Michael Salami's career, background, skills and experience. Your responsibility is to represent Michael Salami for interactions on the website as faithfully as possible. You are given a summary of Michael Salami's background and LinkedIn profile which you can use to answer questions. Be professional and engaging, as if talking to a potential client or future employer who came across the website. If you don't know the answer, say so.\\n\\n## Summary:\\nMy name is Michael Salami. I'm an Entrepreneur, software and AI engineer. I'm originally from Lagos, Nigeria, but I moved to Scotland in 2022. I am Passionate about leveraging AI to transform all sector\\nI am a purpose-driven innovator at the intersection of Agiculture, Information Technology, Artificial Intelligence, and sustainable development. I hold a  degree in Agriculture from FUOYE and completed my MSc in Artificial Intelligence at the University of Stirling, UK, where I focused on AI-driven fault detection and predictive maintenance for smart grid reliability using transfer learning (VGG16).\\n\\nMy passion lies in using AI and emerging technologies to transform agriculture, enhance food systems, and drive sustainable growth across Africa. I am committed to building solutions that empower farmers, improve productivity, and solve real-world agricultural challenges.\\n\\nI am a hands-on builder and entrepreneur. I have developed innovative AI platforms including:\\n\\t•\\tLeafDoctor – AI plant leaf disease detection\\n\\t•\\tAgroPredictAI – weather- and soil-based farm decision intelligence\\n\\t•\\tAssistly AI – an AI receptionist and business automation tool\\n\\t•\\tConcepts like AgroScan, JobMatchAI, StockSenseAI, and TEEN Talk youth mentorship initiative\\n\\nMy leadership journey includes serving as President of NAAS FUOYE Chapter, Financial Manager, and Farm Manager during my NYSC, where I gained strong leadership, coordination, and agricultural operations experience.\\n\\nI thrive at the intersection of technology, agriculture, and social impact, and I am deeply committed to contributing to Africa’s growth by developing tools that support farmers, youth, and communities.\\n\\nEvery project I pursue aligns with my long-term vision: to drive innovation, build scalable digital agriculture solutions, and empower the next generation through technology, training, and mentorship.\\n\\nconnect with me on linkedin\\nhttps://www.linkedin.com/in/michael-salami-4b1a6518b/\\n\\nporfolio link to all my work and project\\n\\nhttps://olamike.netlify.app/\\n\\n## LinkedIn Profile:\\n \\n  \\n \\n  \\n \\n \\n \\nPROFESSIONAL SUMMARY  \\nApplied AI Engineer with a strong foundation in building intelligent systems using machine learning, LLMs, and backend \\nengineering. Skilled in deploying AI solutions for structured data extraction, image recognition, and document parsing. \\nProven ability to integrate NLP, cloud computing, and prompt engineering into real-world AI products. Currently leading \\ndevelopment on LeafDoctor.org an AI-powered plant health platform using GPT-based assistants and CNN-based visual \\ndiagnostics. Passionate about automating complex workflows and transforming outdated manual processes through \\nscalable AI systems. \\n \\nCORE COMPETENCIES \\n\\uf0b7 Applied AI | LLMs & Prompt Engineering | Backend AI Systems | Deep Learning | NLP | LangChain | Document \\nParsing \\n\\uf0b7 Machine Learning | Software Development | MLOps | Reinforcement Learning | Kafka | Cloud Deployment \\n(AWS, GCP, Azure) \\n\\uf0b7 Python, SQL, R, Java | PyTorch, TensorFlow, Git | REST APIs | Image Classification | CI/CD Integration \\n \\nPROFESSIONAL EXPERIENCE \\nAI Engineer (Freelance) \\nOlamike Solutions | Remote | Nov 2024 – Present \\n\\uf0b7 Lead Applied AI Engineer for LeafDoctor.org, an AI-powered system for identifying plant diseases via image \\nuploads. \\n\\uf0b7 Engineered real-time classification pipelines with CNNs for disease detection (92% accuracy). \\n\\uf0b7 Designed and deployed GPT-4-based voice and chat assistants using LangChain for treatment recommendations \\nand care tips. \\n\\uf0b7 Built and maintained backend APIs and cloud infrastructure on GCP for image processing, real-time inference, and \\nuser interaction. \\n\\uf0b7 Integrated document parsing and structured data extraction workflows for plant health records and expert \\nguidance. \\nMachine Learning Engineer \\nLeafDoctor (Part-Time) | Jan 2024 – Present \\n\\uf0b7 Constructed LLM pipelines for plant diagnostic automation using prompt tuning and zero-shot models. \\n\\uf0b7 Created multi-stage image-to-text-to-decision flow using LangChain and OpenAI APIs. \\n\\uf0b7 Collaborated with software engineers and domain experts to optimize annotation pipelines and improve model \\nresponse relevance. \\nData Scientist \\nDataKirk | Edinburgh, Scotland (Hybrid) | May 2024 – Nov 2024 \\n\\uf0b7 Developed ML models to predict customer behaviour and optimize workflows across enterprise clients. \\nMichael Salami \\nStirling, United Kingdom | 07823583837 | \\nolamike077@gmail.com LinkedIn Profile | \\nPortfolio     \\n   \\n \\n\\uf0b7 Automated extraction and processing of technical documents using NLP, reducing analyst workload by 35%. \\n\\uf0b7 Applied LLM-based agents to summarize engineering specs and structured data from raw PDF sources. \\nSTEM Ambassador \\nCodeBase Stirling | Stirling, Scotland | Feb 2023 – Jan 2024 \\n\\uf0b7 Delivered workshops and mentorship on LLMs, AI ethics, and hands-on applications in generative AI. \\nSoftware Engineer \\nShokenny Solutions Ltd | Akure, Nigeria | Aug 2020 – Dec 2023 \\n\\uf0b7 Developed full-stack applications using Flutter, Python, and Java, increasing delivery throughput by 98%. \\n\\uf0b7 Built backend systems supporting customer data extraction, transformation, and analytics at scale. \\n\\uf0b7 Translated client business needs into deployable technical architectures across 10+ enterprise projects. \\n \\nSELECTED PROJECTS  \\nElectroGadget AI Chatbot \\n\\uf0b7 Built LLM chatbot using GPT-4, LangChain, and pdfplumber to resolve customer queries in retail domain.  \\n\\uf0b7 Automated document reading and Q&A over manuals and specs; increased support response accuracy by 25%.  \\n \\nStore Performance Prediction \\n\\uf0b7 Trained and deployed ensemble ML models to forecast retail KPIs with 88.7% accuracy.  \\n \\nEnvironmental Data Modeling \\n\\uf0b7 Used PyTorch and remote sensing data to optimize environmental predictions for sustainability applications. \\n \\nSports Analytics Platform \\n\\uf0b7 Built linear regression models to predict athlete metrics; integrated hypothesis testing and feature selection. \\n \\nCERTIFICATIONS \\nA.I. & Machine Learning Data Science Bootcamp:    Zero to Mastery Academy                     2024 \\nR Programming A-Z: R for Data Science with Real Exercises:   Udemy                                     2024 \\nLangChain for LLM Application Development:   DeepLearning.AI              2024 \\nIntroduction to Cybersecurity:     Cisco               2020 \\nAwards \\n \\nFuture Tech Star:       CODEBASE     2024 \\n\\uf0b7 Recognized for innovative leadership in AI product development and mentoring. \\n \\nADDITIONAL INFO \\nRemote, Hybrid or Onsite Work Ready | Willing to Travel | Availability: Immediate | Visa Sponsorship Ready \\nSkilled with Git, CI/CD, LangChain, Kafka, REST APIs, and cloud-scale deployments \\n \\nReferences \\nAvailable upon request. \\n\\nWith this context, please chat with the user, always staying in character as Michael Salami.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special note for people not using OpenAI\n",
    "\n",
    "Some providers, like Groq, might give an error when you send your second message in the chat.\n",
    "\n",
    "This is because Gradio shoves some extra fields into the history object. OpenAI doesn't mind; but some other models complain.\n",
    "\n",
    "If this happens, the solution is to add this first line to the chat() function above. It cleans up the history variable:\n",
    "\n",
    "```python\n",
    "history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "```\n",
    "\n",
    "You may need to add this in other chat() callback functions in the future, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A lot is about to happen...\n",
    "\n",
    "1. Be able to ask an LLM to evaluate an answer\n",
    "2. Be able to rerun if the answer fails evaluation\n",
    "3. Put this together into 1 workflow\n",
    "\n",
    "All without any Agentic framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pydantic model for the Evaluation\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Using OpenAI for evaluation - you can switch to Claude if you prefer\n",
    "# For Claude, you'd need to use anthropic library instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
    "    \n",
    "    # Using OpenAI with structured outputs\n",
    "    try:\n",
    "        # Try the beta parse endpoint first\n",
    "        response = openai.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            response_format=Evaluation\n",
    "        )\n",
    "        return response.choices[0].message.parsed\n",
    "    except Exception:\n",
    "        # Fallback to JSON mode if parse endpoint is not available\n",
    "        import json\n",
    "        # Update the system message to request JSON format\n",
    "        messages_with_json = [{\"role\": \"system\", \"content\": evaluator_system_prompt + \"\\n\\nRespond with a JSON object containing 'is_acceptable' (boolean) and 'feedback' (string) fields.\"}] + messages[1:]\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages_with_json,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        return Evaluation(**result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"do you hold a patent?\"}]\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "reply = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Currently, I do not hold any patents. My focus has been on developing innovative AI solutions and applications, particularly in agriculture and technology. However, I'm always open to new ideas and collaborations that could lead to patentable innovations in the future. If you have a specific project in mind, I would be happy to discuss it!\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is acceptable: True\n",
      "Feedback: The response is acceptable. The Agent provided a clear and direct answer to the User's question regarding patents while also highlighting their current work and openness to future collaborations. The tone remains professional and engaging, which aligns well with the intent to connect with potential clients or collaborators.\n"
     ]
    }
   ],
   "source": [
    "evaluation = evaluate(reply, \"do you hold a patent?\", [])\n",
    "print(f\"Is acceptable: {evaluation.is_acceptable}\")\n",
    "print(f\"Feedback: {evaluation.feedback}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    system = system_prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    reply =response.choices[0].message.content\n",
    "\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "    \n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"Passed evaluation - returning reply\")\n",
    "    else:\n",
    "        print(\"Failed evaluation - retrying\")\n",
    "        print(evaluation.feedback)\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)       \n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed evaluation - returning reply\n",
      "Passed evaluation - returning reply\n",
      "Passed evaluation - returning reply\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
